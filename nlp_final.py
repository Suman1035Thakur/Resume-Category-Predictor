# -*- coding: utf-8 -*-
"""NLP_Final.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1JKXBQ1wCfV_tsjQJGcpBicaIl7Ux01vF
"""

# ‚úÖ Install any missing libraries (run once)
!pip install xgboost gradio

# ‚úÖ Imports
import os
import pandas as pd
import numpy as np
import seaborn as sns
import re

import nltk
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
from nltk.tokenize import word_tokenize

from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.linear_model import LogisticRegression, PassiveAggressiveClassifier
from sklearn.svm import SVC
from sklearn.naive_bayes import MultinomialNB
from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer
from xgboost import XGBClassifier
from sklearn.metrics import accuracy_score, classification_report
from sklearn.preprocessing import LabelEncoder

# ‚úÖ Download required NLTK data (run once)
nltk.download('punkt')
nltk.download('stopwords')
nltk.download('wordnet')

from google.colab import files
uploaded = files.upload()  # Choose Resume.csv from your computer

import pandas as pd
df = pd.read_csv('/content/Resume.csv')

import pandas as pd
df = pd.read_csv('/content/Resume.csv')

display(df.head())
df.info()

# ‚úÖ Drop unnecessary columns safely
df = df.drop(columns=['ID', 'Resume_html'], errors='ignore')

display(df.head())

import matplotlib.pyplot as plt

df["Category"].value_counts().plot(
    kind="bar",
    figsize=(12,6),
    title="Category Distribution",
    legend=True,
    color="#FFBF00",
    edgecolor="black"
)
plt.xlabel("Categories")
plt.ylabel("Count")
plt.show()

import nltk
nltk.download('punkt', quiet=True)
nltk.download('stopwords', quiet=True)
nltk.download('wordnet', quiet=True)

from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer

# ‚úÖ Initialize stopwords and lemmatizer
stop_words = set(stopwords.words('english'))
lemmatizer = WordNetLemmatizer()

import re
from nltk.tokenize import word_tokenize

# ‚úÖ Ensure tokenizer data is available (in case this cell runs first)
import nltk
nltk.download('punkt_tab', quiet=True)

def clean(text):
    if pd.isna(text):
        return ""
    text = str(text).lower()
    text = re.sub(r'http\S+|www\S+|https\S+', '', text, flags=re.MULTILINE)
    text = re.sub(r'\s+', ' ', text).strip()

    tokens = word_tokenize(text)
    tokens = [lemmatizer.lemmatize(token) for token in tokens if token not in stop_words and len(token) > 2]

    return ' '.join(tokens)

# ‚úÖ Apply cleaning function
df["cleaned"] = df["Resume_str"].apply(clean)

# ‚úÖ Show a few cleaned samples
display(df.head())

from sklearn.preprocessing import LabelEncoder

# Features and target
x = df["cleaned"]
y = df["Category"]

# Encode target labels
le = LabelEncoder()
yn = le.fit_transform(y)

# Optional: check encoding
print(dict(zip(le.classes_, le.transform(le.classes_))))

from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer

# ‚úÖ Vectorize text using TF-IDF
vectorizer = TfidfVectorizer(max_features=5000)  # limit features for speed
X = vectorizer.fit_transform(x)

# ‚úÖ Train-test split
X_train, X_test, Y_train, Y_test = train_test_split(
    X, yn, test_size=0.2, stratify=yn, random_state=42
)

print("Train shape:", X_train.shape)
print("Test shape:", X_test.shape)

from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer

# If you want separate CountVectorizer and TF-IDF from raw text, use original text:
x_raw_train, x_raw_test = train_test_split(x, test_size=0.2, stratify=yn, random_state=42)

# ‚úÖ Count Vectorizer
count_vectorizer = CountVectorizer(max_features=5000)
x_trainc = count_vectorizer.fit_transform(x_raw_train)
x_testc = count_vectorizer.transform(x_raw_test)

# ‚úÖ TF-IDF Vectorizer
tfidf_vectorizer = TfidfVectorizer(max_features=5000)
x_traint = tfidf_vectorizer.fit_transform(x_raw_train)
x_testt = tfidf_vectorizer.transform(x_raw_test)

print("CountVectorizer train shape:", x_trainc.shape)
print("TF-IDF train shape:", x_traint.shape)

from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.linear_model import LogisticRegression, PassiveAggressiveClassifier
from sklearn.svm import SVC
from sklearn.naive_bayes import MultinomialNB
from xgboost import XGBClassifier

# ‚úÖ Define models
models = {
    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42),
    'Logistic Regression': LogisticRegression(random_state=42, max_iter=1000),
    'SVM': SVC(random_state=42),
    'Naive Bayes': MultinomialNB(),
    'XGBoost': XGBClassifier(use_label_encoder=False, eval_metric='mlogloss', random_state=42),  # Colab may warn, ignore
    'Gradient Boosting': GradientBoostingClassifier(random_state=42),
    'Passive Aggressive': PassiveAggressiveClassifier(max_iter=1000, random_state=42)
}

# ‚úÖ Optional: print models
print("Models ready:", list(models.keys()))

from xgboost import XGBClassifier
from sklearn.metrics import accuracy_score, classification_report

xgb = XGBClassifier(random_state=42, n_estimators=200, learning_rate=0.1, max_depth=6)

print("Training XGBoost with TF-IDF features...")
xgb.fit(x_traint, Y_train)

preds = xgb.predict(x_testt)
acc = accuracy_score(Y_test, preds)
print(f"\n‚úÖ XGBoost Accuracy: {acc:.4f}")
print(classification_report(Y_test, preds, target_names=le.classes_))

import numpy as np
import gradio as gr

# Refit TF-IDF on full data for better generalization
tfidf_vectorizer_full = TfidfVectorizer(max_features=5000)
X_full_tfidf = tfidf_vectorizer_full.fit_transform(df["cleaned"])
y_full = yn

# Retrain model on all data
xgb.fit(X_full_tfidf, y_full)

def predict_resume_category(text):
    cleaned_text = clean(text)
    vectorized_text = tfidf_vectorizer_full.transform([cleaned_text])

    # Get prediction probabilities
    probs = xgb.predict_proba(vectorized_text)[0]
    top_indices = np.argsort(probs)[::-1][:3]
    top_labels = le.inverse_transform(top_indices)
    top_probs = probs[top_indices]

    result = "üîç **Top 3 Predicted Categories:**\n"
    for label, prob in zip(top_labels, top_probs):
        result += f"- {label}: {prob*100:.2f}%\n"
    return result

iface = gr.Interface(
    fn=predict_resume_category,
    inputs=gr.Textbox(lines=10, placeholder="Paste resume text here..."),
    outputs="markdown",
    title="Resume Category Predictor",
    description="Paste resume text to predict its most likely job categories using XGBoost + TF-IDF."
)

iface.launch(share=True)

from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, classification_report

# Initialize Logistic Regression with balanced classes
lr = LogisticRegression(max_iter=1000, class_weight='balanced', random_state=42)

# Train on TF-IDF vectors
lr.fit(x_traint, Y_train)

# Evaluate on test set
preds = lr.predict(x_testt)
acc = accuracy_score(Y_test, preds)
print(f"‚úÖ Logistic Regression Accuracy: {acc:.4f}")
print(classification_report(Y_test, preds, target_names=le.classes_))

import gradio as gr

def predict_resume_category_single(text):
    cleaned_text = clean(text)
    vectorized_text = tfidf_vectorizer.transform([cleaned_text])

    # Predict single category
    pred_class = lr.predict(vectorized_text)[0]
    category = le.inverse_transform([pred_class])[0]

    return f"üßæ Predicted Resume Category: {category}"

# Build Gradio interface
iface = gr.Interface(
    fn=predict_resume_category_single,
    inputs=gr.Textbox(lines=10, placeholder="Paste resume text here..."),
    outputs="text",
    title="Resume Category Predictor",
    description="Paste resume text to predict its most likely job category."
)

iface.launch(share=True)

!pip install -q sentence-transformers

from sentence_transformers import SentenceTransformer
import numpy as np

# Load Sentence-BERT model
model_sbert = SentenceTransformer('all-MiniLM-L6-v2')

# Convert all cleaned resumes to embeddings
X_embeddings = model_sbert.encode(df['cleaned'].tolist(), show_progress_bar=True)
y_labels = yn  # your encoded labels

from sklearn.model_selection import train_test_split

X_train, X_test, Y_train, Y_test = train_test_split(
    X_embeddings, y_labels, test_size=0.2, stratify=y_labels, random_state=42
)

from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, classification_report

lr_sbert = LogisticRegression(max_iter=1000, class_weight='balanced', random_state=42)
lr_sbert.fit(X_train, Y_train)

# Evaluate
preds = lr_sbert.predict(X_test)
print(f"‚úÖ Accuracy: {accuracy_score(Y_test, preds):.4f}")
print(classification_report(Y_test, preds, target_names=le.classes_))

!pip install -q PyPDF2

import gradio as gr
import PyPDF2

# Prediction function
def predict_resume(input_text, file):
    if file is not None:
        pdf_reader = PyPDF2.PdfReader(file.name)
        text = ""
        for page in pdf_reader.pages:
            text += page.extract_text() + " "
    else:
        text = input_text

    if not text.strip():
        return "Please provide resume text or upload a PDF file."

    cleaned_text = clean(text)
    embedding = model_sbert.encode([cleaned_text])
    pred_class = lr_sbert.predict(embedding)[0]
    category = le.inverse_transform([pred_class])[0]

    return f"Predicted Category: {category}"

# Modern monochrome theme
theme = gr.themes.Soft(
    primary_hue="slate",
    secondary_hue="gray",
    neutral_hue="slate",
    font=gr.themes.GoogleFont("Inter")
).set(
    body_background_fill="#ffffff",
    body_text_color="#0f172a",
    button_primary_background_fill="#1e293b",
    button_primary_background_fill_hover="#0f172a",
    button_primary_text_color="#ffffff",
    input_background_fill="#f8fafc",
    input_border_color="#cbd5e1",
    block_background_fill="#ffffff",
    block_border_color="#e2e8f0",
    block_shadow="0 1px 3px 0 rgba(0, 0, 0, 0.1)",
)

# Enhanced custom CSS with animations
custom_css = """
@keyframes fadeInUp {
    from {
        opacity: 0;
        transform: translateY(30px);
    }
    to {
        opacity: 1;
        transform: translateY(0);
    }
}

@keyframes slideInLeft {
    from {
        opacity: 0;
        transform: translateX(-30px);
    }
    to {
        opacity: 1;
        transform: translateX(0);
    }
}

@keyframes pulse {
    0%, 100% {
        opacity: 1;
    }
    50% {
        opacity: 0.8;
    }
}

.gradio-container {
    max-width: 1200px !important;
    margin: auto !important;
}

#header {
    text-align: center;
    padding: 2rem 0;
    background: #1e293b;
    border-radius: 1rem;
    margin-bottom: 2rem;
    box-shadow: 0 4px 6px -1px rgba(0, 0, 0, 0.1);
    animation: fadeInUp 0.8s ease-out;
}

#header h1 {
    color: #ffffff;
    font-size: 2.5rem;
    font-weight: 700;
    margin: 0;
}

#header p {
    color: #cbd5e1;
    font-size: 1.1rem;
    margin-top: 0.5rem;
    font-weight: 400;
}

.info-tiles {
    display: grid;
    grid-template-columns: repeat(auto-fit, minmax(250px, 1fr));
    gap: 1.5rem;
    margin: 2rem 0;
}

.tile {
    background: #ffffff;
    border: 1px solid #e2e8f0;
    border-radius: 0.75rem;
    padding: 1.5rem;
    box-shadow: 0 1px 3px 0 rgba(0, 0, 0, 0.1);
    transition: all 0.3s ease;
    animation: fadeInUp 0.8s ease-out;
}

.tile:hover {
    transform: translateY(-5px);
    box-shadow: 0 10px 15px -3px rgba(0, 0, 0, 0.1);
    border-color: #cbd5e1;
}

.tile h3 {
    color: #1e293b;
    font-size: 1.1rem;
    font-weight: 600;
    margin: 0 0 0.5rem 0;
}

.tile p {
    color: #64748b;
    font-size: 0.9rem;
    margin: 0;
    line-height: 1.5;
}

.input-section {
    background: #ffffff;
    padding: 2rem;
    border-radius: 1rem;
    border: 1px solid #e2e8f0;
    box-shadow: 0 1px 3px 0 rgba(0, 0, 0, 0.1);
    animation: slideInLeft 0.8s ease-out;
    transition: all 0.3s ease;
}

.input-section:hover {
    box-shadow: 0 4px 6px -1px rgba(0, 0, 0, 0.1);
}

.output-section {
    background: #ffffff;
    padding: 2rem;
    border-radius: 1rem;
    border: 1px solid #e2e8f0;
    margin-top: 1.5rem;
    box-shadow: 0 1px 3px 0 rgba(0, 0, 0, 0.1);
    animation: fadeInUp 1s ease-out;
    transition: all 0.3s ease;
}

.output-section:hover {
    box-shadow: 0 4px 6px -1px rgba(0, 0, 0, 0.1);
}

.feature-box {
    background: #f8fafc;
    border: 1px solid #e2e8f0;
    border-radius: 0.75rem;
    padding: 1rem;
    margin: 1rem 0;
    animation: fadeInUp 1.2s ease-out;
}

.feature-box p {
    color: #475569;
    margin: 0;
    font-size: 0.95rem;
    font-weight: 400;
}

#submit-btn {
    font-size: 1.1rem !important;
    padding: 0.75rem 2rem !important;
    font-weight: 600 !important;
    border-radius: 0.5rem !important;
    transition: all 0.3s ease !important;
}

#submit-btn:hover {
    transform: translateY(-2px);
    box-shadow: 0 4px 6px -1px rgba(0, 0, 0, 0.2);
}

#submit-btn:active {
    transform: translateY(0);
}

footer {
    text-align: center;
    margin-top: 2rem;
    padding: 1rem;
    color: #94a3b8;
}
"""

# Create interface
with gr.Blocks(theme=theme, css=custom_css, title="Resume Category Predictor") as iface:
    # Header
    with gr.Row(elem_id="header"):
        gr.Markdown(
            """
            <h1>Resume Category Predictor</h1>
            <p>Instantly classify resumes using AI-powered analysis</p>
            """
        )

    # Info tiles
    with gr.Row():
        gr.HTML(
            """
            <div class="info-tiles">
                <div class="tile">
                    <h3>Fast Analysis</h3>
                    <p>Get instant predictions powered by advanced machine learning models trained on thousands of resumes.</p>
                </div>
                <div class="tile">
                    <h3>Multiple Formats</h3>
                    <p>Support for both text input and PDF uploads with automatic text extraction capabilities.</p>
                </div>
                <div class="tile">
                    <h3>Accurate Results</h3>
                    <p>High accuracy categorization using state-of-the-art natural language processing techniques.</p>
                </div>
            </div>
            """
        )

    # Main content
    with gr.Row():
        with gr.Column(scale=1):
            # Input section
            with gr.Group(elem_classes="input-section"):
                gr.Markdown("### Input Your Resume")

                with gr.Tabs():
                    with gr.Tab("Paste Text"):
                        input_text = gr.Textbox(
                            lines=12,
                            placeholder="Paste your resume content here...\n\nInclude details like:\n‚Ä¢ Work experience\n‚Ä¢ Skills\n‚Ä¢ Education\n‚Ä¢ Projects",
                            label="Resume Text",
                            show_label=False
                        )

                    with gr.Tab("Upload PDF"):
                        input_file = gr.File(
                            file_types=[".pdf"],
                            label="Upload PDF Resume",
                            show_label=False
                        )
                        gr.Markdown(
                            """
                            <div class="feature-box">
                            <p>Tip: Upload a PDF file for automatic text extraction</p>
                            </div>
                            """
                        )

                submit_btn = gr.Button(
                    "Predict Category",
                    variant="primary",
                    size="lg",
                    elem_id="submit-btn"
                )

            # Output section
            with gr.Group(elem_classes="output-section"):
                gr.Markdown("### Prediction Result")
                output_text = gr.Textbox(
                    label="Category",
                    placeholder="Your prediction will appear here...",
                    show_label=False,
                    interactive=False,
                    lines=3
                )

    # Info section
    with gr.Row():
        with gr.Column():
            gr.Markdown(
                """
                <div class="feature-box">
                <p>
                How it works: Our AI model analyzes your resume content and predicts the most suitable job category.
                Simply paste your resume text or upload a PDF file, then click the predict button.
                </p>
                </div>
                """
            )

    # Event handlers
    submit_btn.click(
        fn=predict_resume,
        inputs=[input_text, input_file],
        outputs=output_text
    )

    # Also trigger on Enter key in text input
    input_text.submit(
        fn=predict_resume,
        inputs=[input_text, input_file],
        outputs=output_text
    )

# Launch the app
if __name__ == "__main__":
    iface.launch(share=True)